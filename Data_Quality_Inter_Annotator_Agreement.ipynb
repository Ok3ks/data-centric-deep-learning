{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ok3ks/data-centric-deep-learning/blob/main/Data_Quality_Inter_Annotator_Agreement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Quality: Inter-Annotator Agreement\n",
        "\n",
        "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive."
      ],
      "metadata": {
        "id": "is5Qs-0rpUSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In this notebook, we will focus on measuring the quality of annotations. Recall from the course notes that because we can't exactly compute the accuracy of annotations (we have no ground truth), we settle for computing the reliability of annotations: how consistent are annotators with each other? We use this measure of agreement as a proxy for a measure of quality. \n",
        "\n",
        "## Goals\n",
        "\n",
        "The main goal is to understand how to write code to compute inter-annotator agreement. In particular, we will\n",
        "- Study annotation within two different domains: classification and object detection.\n",
        "- Learn how to code Cohen's Kappa, a measure of agreement that takes into account the probability of agreement under guessing.\n",
        "- Learn how to implement the *Intersection over Union* metric, or IoU.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. We provide starter code and data to give your work a common starting point and scaffolding. You should try to keep function signatures unchanged to support any later usage or grading of your project.\n",
        "1. Ensure you read through the document and starting code before beginning your work. Understand the overall structure and goals of the project to make your implementation efficient. "
      ],
      "metadata": {
        "id": "nFdqr7SpnZBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "SP0v1HNToXxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlGSOjKjeHlS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from math import isclose\n",
        "from sklearn.metrics import cohen_kappa_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification\n",
        "\n",
        "Let's start with binary classification, which we know and love from sentiment analysis. In the reading, we learned about Cohen's kappa, a score measuring the level of agreement between two annotators that accounts for the probability of randomly agreeing with one another. \n",
        "\n",
        "### Why do we use Cohen's kappa instead of just measuring agreement directly? \n",
        "\n",
        "Let's find out using a simulation! Let's pretend to be annotator 1 and we will generate their labels randomly. Let's say class 0 occurs roughly 90% of the time. So we can produce a random number between 0 and 1 one thousand times and set the annotation to 1 if the number is greater than 0.9, otherwise set the annotation to 0."
      ],
      "metadata": {
        "id": "S1CrFVjs7WGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# randomly generate annotator one's labels\n",
        "annotations1 = np.random.rand(1000) > 0.9"
      ],
      "metadata": {
        "id": "HT-4zDyg8S1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For annotator two, we simulate some error but flipping half of the labels where annotator one gave a positive label."
      ],
      "metadata": {
        "id": "e9Up-0TG-sfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotations2 = np.copy(annotations1)\n",
        "\n",
        "# find where annotator one predicted positive\n",
        "indices = np.where(annotations2 == 1)[0]\n",
        "indices = np.random.choice(indices, size=int(len(indices) / 2))\n",
        "\n",
        "# flip half of them!\n",
        "annotations2[indices] = 1 - annotations2[indices]"
      ],
      "metadata": {
        "id": "Hkwwy8ui-qgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute the raw agreement just as average accuracy between the two annotators."
      ],
      "metadata": {
        "id": "2gH3lzxA-_tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_agreement = float(np.mean(annotations1 == annotations2))\n",
        "print(f'Agreement in Accuracy: {acc_agreement}')"
      ],
      "metadata": {
        "id": "f-IMfHqh9tQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! 96% agreement is pretty good between annotators, right? Let's see what Cohen's Kappa says (using `scikit-learn`)."
      ],
      "metadata": {
        "id": "r5NM6hZH_I77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kappa_agreement = cohen_kappa_score(annotations1, annotations2)\n",
        "print(f'Agreement in Cohen\\'s Kappa: {kappa_agreement}')"
      ],
      "metadata": {
        "id": "XvGMKliw-DHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait, this is a lot lower. 75% agreement is much less acceptable of a level of agreement for practical applications. Why is Cohen's Kappa so low whereas accuracy is so high? \n",
        "\n",
        "It's because the expected agreement of annotators guessing each class is very skewed. Consider:\n"
      ],
      "metadata": {
        "id": "VjFO4UoT_Zec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_zero_1 = np.sum(annotations1 == 0)\n",
        "num_one_1 = np.sum(annotations1 == 1)\n",
        "\n",
        "num_zero_2 = np.sum(annotations2 == 0)\n",
        "num_one_2 = np.sum(annotations2 == 1)\n",
        "\n",
        "print(f'Annotator 1 labeled {num_zero_1} zeros and {num_one_1} ones.')\n",
        "print(f'Annotator 2 labeled {num_zero_2} zeros and {num_one_2} ones.')"
      ],
      "metadata": {
        "id": "BpakK7laALI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like both annotators are very biased towards labeling class zero meaning that purely competing accuracy is over-optimistic on the agreement: a lot of the time, annotator one and two likely just agree by chance. Cohen's Kappa accounts for the bias to class zero. \n",
        "\n",
        "### Let's implement it!\n",
        "\n",
        "To compute Cohen's Kappa, we need to compute two terms: `p_0` and `p_e`. \n",
        "Let `p_0` be the observed agreement between rater i.e. what fraction of the time did annotators one and two agree? This is just like computing accuracy as we did above. \n",
        "\n",
        "Let `p_e` be the probability of chance agreement. This slightly trick to compute. First, let `p_e = p_e1 + p_e2` where `p_e1 = n1 * m1 / N^2` where `N` is the total number of examples (for us 1000), and `n1` is the number of times annotator 1 predicted class 1 and `m1` is the number of times annotator 2 predicted class 1. Similarly `p_e2 = n2 * m2 / N^2` where `n2` is the number of times annotator 1 predicted class 2. And so on... See [Wikipedia](https://en.wikipedia.org/wiki/Cohen%27s_kappa) for a great summary."
      ],
      "metadata": {
        "id": "ly0tE9slAd06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def our_cohen_kappa_score(y1, y2):\n",
        "  r\"\"\"Cohen’s kappa: a statistic that measures inter-annotator agreement.\n",
        "\n",
        "  This function computes Cohen’s kappa [1], a score that expresses the level \n",
        "  of agreement between two annotators on a classification problem. \n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  y1 (array): labels assigned by first annotator.\n",
        "  y2 (array): labels assigned by second annotator.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  kappa: float (between -1 and 1)\n",
        "  \"\"\"\n",
        "  assert y1.shape[0] == y2.shape[0], \"Size is not the same.\"\n",
        "  p0 = None\n",
        "  pe = None\n",
        "  \n",
        "  # this is total length of `y1`\n",
        "  N = float(y1.shape[0])\n",
        "  # example of how to compute `n1` and `m1` (class 0)\n",
        "  # you will need to add similar variables for class 1\n",
        "  n1 = np.sum(y1 == 0)\n",
        "  m1 = np.sum(y2 == 0)\n",
        "\n",
        "  # ==========================\n",
        "  # FILL ME OUT\n",
        "  # Compute p0 and pe as described above.\n",
        "  # \n",
        "  # Pseudocode:\n",
        "  # --\n",
        "  # p0 = ...\n",
        "  # n2 = ...\n",
        "  # m2 = ...\n",
        "  # pe = ...\n",
        "  # \n",
        "  # Type:\n",
        "  # --\n",
        "  # p0: float\n",
        "  # n2: float\n",
        "  # m2: float\n",
        "  # pe: float\n",
        "  # ==========================\n",
        "  kappa = (p0 - pe) / (1.0 - pe)\n",
        "  return kappa"
      ],
      "metadata": {
        "id": "TPjGkIHnA7qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test our implementation by comparing to scikit-learn's implementation."
      ],
      "metadata": {
        "id": "7G8GE0M-DLtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_kappa = our_cohen_kappa_score(annotations1, annotations2)\n",
        "scikit_kappa = cohen_kappa_score(annotations1, annotations2)\n",
        "\n",
        "print(f'Our kappa: {our_kappa} | Scikit-Learn kappa: {scikit_kappa}')\n",
        "assert isclose(our_kappa, scikit_kappa), \"Not close... next time!\" "
      ],
      "metadata": {
        "id": "gFIszlJJDUQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "v0oQ1FyfHUU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection: Intersection over Union\n",
        "\n",
        "Recall from the reading that for object detection, the annotations are bounding boxes that specify a `(x,y)` coordinate along with a height `h` and a width `w`. To measure agreement, we will implement IoU. You might be wondering: is there such a thing as Cohen's Kappa for IoU? In practice, this isn't used  because the random chance of selecting the right bounding box out of all possible bounding boxes is neglible. "
      ],
      "metadata": {
        "id": "lsWmHuP8qRqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1-IBa5lOggQQ558m5ElChN6WUC3m63ncW\n",
        "!gdown --id 1cpszI376NlM3yEGyaaC9WsMkTacEebzK\n",
        "!unzip -o images.zip"
      ],
      "metadata": {
        "id": "IG0Y1O7oqUy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls images   # there are 10 images!"
      ],
      "metadata": {
        "id": "_UAyvgxyLyaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out the annotations file!"
      ],
      "metadata": {
        "id": "B0cVaDNBL2t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./bbox-anno.json', 'r') as fp:\n",
        "  annotations = json.load(fp)\n",
        "print(f'There are {len(annotations)} annotations, two for each image.')"
      ],
      "metadata": {
        "id": "slMwAMDAL9b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each annotations is a complex data structure. Let's focus on a few attributes. "
      ],
      "metadata": {
        "id": "EleqpKJYMQxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotation = annotations[0]        # grab the first annotation\n",
        "annotation_id = annotation['id']   # {annotation_id}.png will be the filename\n",
        "print(f'{annotation_id}.png')"
      ],
      "metadata": {
        "id": "Xb2mQBiFMCC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each annotation could contain many bounding boxes if there are multiple objects in the scene!"
      ],
      "metadata": {
        "id": "bve0f7RUO906"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotation"
      ],
      "metadata": {
        "id": "KCnflgKcTVsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = annotation['label']\n",
        "print(f'Number of annotated bounding boxes: {len(labels)}')"
      ],
      "metadata": {
        "id": "FM45Dl8fOyek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what an annotation looks like. As you might have expected, there is a `x`, `y`, `width` and `height`. You can ignore the rest of the fields. "
      ],
      "metadata": {
        "id": "otGeXeP5PCQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0]"
      ],
      "metadata": {
        "id": "kDCMhCE8PFle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we get to the code, let's visualize a few of these!"
      ],
      "metadata": {
        "id": "72opT73dPUHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "hzDKGZK-PWy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im = Image.open(f'images/{annotation_id}.png')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 10))\n",
        "ax.imshow(im)  # viz image\n",
        "\n",
        "labels = annotation['label']\n",
        "for bbox in labels:\n",
        "  # the data stored in `x`, `y`, `width`, and `height` are percentages from \n",
        "  # 0-100, meaning to get the actual coordinates, we need to multiple them by\n",
        "  # the original height or width.\n",
        "  x = bbox['x'] / 100. * bbox['original_width']\n",
        "  y = bbox['y'] / 100. * bbox['original_height']\n",
        "  w = bbox['width'] / 100. * bbox['original_width']\n",
        "  h = bbox['height'] / 100. * bbox['original_height']\n",
        "  rect = patches.Rectangle((x, y), w, h, linewidth=3, \n",
        "                            edgecolor='r', facecolor='none')\n",
        "  ax.add_patch(rect)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k4K7Ep7EPgOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualize the second annotation on the same image."
      ],
      "metadata": {
        "id": "wXxEejqyWH-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im = Image.open(f'images/{annotation_id}.png')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 10))\n",
        "ax.imshow(im)  # viz image\n",
        "\n",
        "labels = annotations[0]['label']\n",
        "for bbox in labels:\n",
        "  x = bbox['x'] / 100. * bbox['original_width']\n",
        "  y = bbox['y'] / 100. * bbox['original_height']\n",
        "  w = bbox['width'] / 100. * bbox['original_width']\n",
        "  h = bbox['height'] / 100. * bbox['original_height']\n",
        "  rect = patches.Rectangle((x, y), w, h, linewidth=3, \n",
        "                            edgecolor='r', facecolor='none')\n",
        "  ax.add_patch(rect)\n",
        "\n",
        "labels = annotations[1]['label']\n",
        "for bbox in labels:\n",
        "  x = bbox['x'] / 100. * bbox['original_width']\n",
        "  y = bbox['y'] / 100. * bbox['original_height']\n",
        "  w = bbox['width'] / 100. * bbox['original_width']\n",
        "  h = bbox['height'] / 100. * bbox['original_height']\n",
        "  rect = patches.Rectangle((x, y), w, h, linewidth=3, \n",
        "                            edgecolor='g', facecolor='none')\n",
        "  ax.add_patch(rect)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zVFgi9-sUJsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how there are slightly differences in the annnotation due to interpretation. Annotator one (red) only selected the face whereas annotator two (green) included the hat, both of which are reasonable approaches!"
      ],
      "metadata": {
        "id": "l6BbuB7IWTDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 14\n",
        "im = Image.open(f'images/{annotations[index][\"id\"]}.png')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.imshow(im) \n",
        "\n",
        "labels = annotations[index]['label']\n",
        "for bbox in labels:\n",
        "  x = bbox['x'] / 100. * bbox['original_width']\n",
        "  y = bbox['y'] / 100. * bbox['original_height']\n",
        "  w = bbox['width'] / 100. * bbox['original_width']\n",
        "  h = bbox['height'] / 100. * bbox['original_height']\n",
        "  rect = patches.Rectangle((x, y), w, h, linewidth=3, \n",
        "                            edgecolor='r', facecolor='none')\n",
        "  ax.add_patch(rect)\n",
        "\n",
        "# visualize the other annotator's labels\n",
        "labels = annotations[index+1]['label']\n",
        "for bbox in labels:\n",
        "  x = bbox['x'] / 100. * bbox['original_width']\n",
        "  y = bbox['y'] / 100. * bbox['original_height']\n",
        "  w = bbox['width'] / 100. * bbox['original_width']\n",
        "  h = bbox['height'] / 100. * bbox['original_height']\n",
        "  rect = patches.Rectangle((x, y), w, h, linewidth=3, \n",
        "                            edgecolor='g', facecolor='none')\n",
        "  ax.add_patch(rect)"
      ],
      "metadata": {
        "id": "q5nmT3XgWRdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These bounding boxes can be quite complicated! Here's an example of an image with many faces. Notice that the annotator in green captured some faces that the annotator in red did not. Computing IoU in these contexts can be more complex! Let's go through it together 💯"
      ],
      "metadata": {
        "id": "cFFVkQEwW9pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing IoU\n",
        "\n",
        "Recall from the reading that IoU is computed as the intersection between two annotated bounding boxes over the union. But what if you have more than one object in a scene like the photo above? For a given bounding box provided by annotator A, compute the IoU with bounding boxes for all labels from annotator B and report the largest one! In other words, loop through all of annotator B's bounding boxes and find the one that most likely describes the same object. \n",
        "\n",
        "Below we provide the scaffold for the implementation of IoU with a portion for you to complete. Then, we will run it over all ten images to compute agreement between the two annotators!"
      ],
      "metadata": {
        "id": "ZDrmYh5qXLRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(bbox1, bbox2):\n",
        "  r\"\"\"Compute the IoU of two bounding boxes.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  bbox1 (dict): a bounding box object.\n",
        "    keys: x, y, height, width (all represent percentages)\n",
        "          original_width, original_height \n",
        "  bbox2 (dict): identical to bbox1.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  iou (float): score between 0 and 1.\n",
        "  \"\"\"\n",
        "  # first thing to do is convert percentages to points!\n",
        "  bb1 = {\n",
        "      # (x1, y1) represents the top left point of the bbox\n",
        "      'x1': bbox1['x'] / 100. * bbox1['original_width'],\n",
        "      'y1': bbox1['y'] / 100. * bbox1['original_height'],\n",
        "      # instead of height and width, we will compute the bottom right corner\n",
        "      'x2': (bbox1['x'] / 100. * bbox1['original_width'] + \n",
        "             bbox1['width'] / 100. * bbox1['original_width']),\n",
        "      'y2': (bbox1['y'] / 100. * bbox1['original_height'] + \n",
        "             bbox1['height'] / 100. * bbox1['original_height']),\n",
        "  }\n",
        "  bb2 = {\n",
        "      'x1': bbox2['x'] / 100. * bbox2['original_width'],\n",
        "      'y1': bbox2['y'] / 100. * bbox2['original_height'],\n",
        "      'x2': (bbox2['x'] / 100. * bbox2['original_width'] + \n",
        "             bbox2['width'] / 100. * bbox2['original_width']),\n",
        "      'y2': (bbox2['y'] / 100. * bbox2['original_height'] + \n",
        "             bbox2['height'] / 100. * bbox2['original_height']),\n",
        "  }\n",
        "  \n",
        "  # sanity check!\n",
        "  assert bb1['x1'] < bb1['x2']\n",
        "  assert bb1['y1'] < bb1['y2']\n",
        "  assert bb2['x1'] < bb2['x2']\n",
        "  assert bb2['y1'] < bb2['y2']\n",
        "\n",
        "  x_left = max(bb1['x1'], bb2['x1'])\n",
        "  y_top = max(bb1['y1'], bb2['y1'])\n",
        "  x_right = min(bb1['x2'], bb2['x2'])\n",
        "  y_bottom = min(bb1['y2'], bb2['y2'])\n",
        "\n",
        "  if x_right < x_left or y_bottom < y_top:\n",
        "    return 0.0  # punt on unexpected result\n",
        "\n",
        "  # area of intersection: intersection of two axis-aligned bounding boxes \n",
        "  # is itself an axis-aligned bounding box\n",
        "  intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "\n",
        "  # compute area of individual bounding boxes\n",
        "  area1 = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n",
        "  area2 = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n",
        "\n",
        "  iou = None\n",
        "  # =================================\n",
        "  # FILL ME OUT\n",
        "  # \n",
        "  # Use `intersection`, `area1`, and `area2` to compute IoU. Try to \n",
        "  # draw a Venn diagram to see how to compute the union from these \n",
        "  # three variables! Careful not to count the intersection twice.\n",
        "  # \n",
        "  # Pseudocode:\n",
        "  # --\n",
        "  # iou = ...\n",
        "  #\n",
        "  # Type:\n",
        "  # --\n",
        "  # iou: float\n",
        "  # =================================\n",
        "  assert iou >= 0 and iou <= 1\n",
        "\n",
        "  return iou"
      ],
      "metadata": {
        "id": "9VMhXSV3Wppr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the implementation on the first image we looked at above (40.png), which has only one bounding box. "
      ],
      "metadata": {
        "id": "eQiaqv6WfEyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert annotations[0]['id'] == 40\n",
        "assert annotations[1]['id'] == 40\n",
        "\n",
        "bbox1 = annotations[0]['label'][0]\n",
        "bbox2 = annotations[1]['label'][0]"
      ],
      "metadata": {
        "id": "yqWjo70-bbkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if the implementation is correct, this should not raise an exception!\n",
        "assert isclose(compute_iou(bbox1, bbox2), 0.6212590299277606)"
      ],
      "metadata": {
        "id": "oC8KXURpfNGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok but now what do we do if we have multiple objects in a scene? You don't need to implement anything below but we encourage you to walk through the steps to check your understanding."
      ],
      "metadata": {
        "id": "ISnlm0T1fyO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou_multiple_objects(bboxes1, bboxes2):\n",
        "  r\"\"\"Compute IoU given multiple objects in a scene.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  bboxes1 (list[dict]): list of bounding box objects.\n",
        "    keys: x, y, height, width (all represent percentages)\n",
        "          original_width, original_height \n",
        "  bboxes2 (dict): identical to bboxes1.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  avg_iou (float): score between 0 and 1 representing the \n",
        "    average iou over all objects in the scene. \n",
        "  \"\"\"\n",
        "  all_iou = []\n",
        "  for bbox1 in bboxes1:\n",
        "    cache = []\n",
        "    for bbox2 in bboxes2:\n",
        "      # we reuse your implementation! yay!\n",
        "      cur_iou = compute_iou(bbox1, bbox2)\n",
        "      cache.append(cur_iou)\n",
        "    all_iou.append(max(cache))\n",
        "  return float(np.mean(all_iou))"
      ],
      "metadata": {
        "id": "DpzWhOdVfdip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try computing it on the image with lots of people in a crowd."
      ],
      "metadata": {
        "id": "KkJHHmlnh-Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bboxes1 = annotations[14]['label']\n",
        "bboxes2 = annotations[15]['label']\n",
        "\n",
        "iou = compute_iou_multiple_objects(bboxes1, bboxes2)\n",
        "assert isclose(iou, 0.6047251333091429)"
      ],
      "metadata": {
        "id": "KNaBhaV2gGrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay! amazing, we are now ready to compute average IoU agreement over all images in our mini-dataset."
      ],
      "metadata": {
        "id": "VdWbRvvhiYVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# indices of ids for annotation 1\n",
        "indices1 = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
        "ious = []  # store iou's here\n",
        "\n",
        "for index in indices1:\n",
        "  bboxes1 = annotations[index]['label']    # annotation 1's bboxes\n",
        "  bboxes2 = annotations[index+1]['label']  # annotation 2's bboxes\n",
        "\n",
        "  iou = compute_iou_multiple_objects(bboxes1, bboxes2)\n",
        "  ious.append(iou)\n",
        "\n",
        "print(f'IoU: {np.mean(ious)} +/- {np.std(ious)}')"
      ],
      "metadata": {
        "id": "QyAlr1PniHGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is tempting to treat this as we would with accuracy, in which case 0.65 seems small. But IoU is a wholly different metric and an IoU over 0.5 is generally considered a good agreement. In practice, we would be quite happy with 0.65 given its healthy margin over 0.5."
      ],
      "metadata": {
        "id": "TdMsxSFWjG0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# That's it folks!\n",
        "\n",
        "Just a quite reminder of what we covered. In this notebook:\n",
        "\n",
        "- You implemented Cohen's Kappa, an important metric for measuring agreement more fairly in classification settings. \n",
        "\n",
        "- You got to see bounding boxes in modern object detection settings. \n",
        "\n",
        "- You implemented Intersection over Union for annotations in a realistic facial bounding box task.\n",
        "\n",
        "Great work! This is the last portion of your project for week 1. By now, you've learned a lot about annotation and what makes it both important and difficult. If you have any questions, please feel free to ask the teaching team! Otherwise, get ready for week 2 ⭐"
      ],
      "metadata": {
        "id": "_cVaiHEsjV1c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxFYOWKijkqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
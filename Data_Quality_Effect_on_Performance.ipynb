{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Quality: Effect of Data Quality on Performance\n",
        "\n",
        "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
        "\n",
        "What exactly is the impact of data error and annotation error on performance? To answer this, we will return to our trusty Stanford Sentiment Treebank (SST) dataset (we saw this before in the Deep Learning Refresher), and measure it ourselves! \n",
        "\n",
        "Now we put on our hat as an evil adversary whose job is to reduce data quality. What happens when we corrupt the inputs more and more? What happens when we corrupt the annotations? What is the magnitude of impact on test accuracy?\n",
        "\n",
        "One of the sub-goals of the course is to see the same concepts across different data modalities to provide student's a broader learning landscape. Towards this goal, we will also run an analogous experiment in an image context using the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, a popular benchmark dataset for image classification. \n",
        "\n",
        "### Setup\n",
        "\n",
        "You will need your a GPU for this notebook. Click Runtime in the Colab menu and change your runtype. \n",
        "\n",
        "### Install Dependencies"
      ],
      "metadata": {
        "id": "zerzrCMhZiLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install jsonlines\n",
        "!pip install tqdm\n",
        "!pip install datasets\n",
        "!pip install pytorch-lightning\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "RXMiIqRrcIJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's get started!\n",
        "\n",
        "To begin, we will load our libraries, and download the SST data and pre-computed features. If you haven't seen the [Deep Learning Refresher](https://colab.research.google.com/drive/1RzpqXcNnCqdczkEG8El51chYsu8umscw), it would be worth reviewing as some of the code here is based on that notebook."
      ],
      "metadata": {
        "id": "pV6iYh8McKvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "def seed_everything(seed, use_cuda=True):\n",
        "  # so results are reproducible!\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if use_cuda: \n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)"
      ],
      "metadata": {
        "id": "dVuZgWWZZi4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download our BERT feature bank on SST from the Deep Learning Refresher. As a reminder, we pre-computed BERT features (size 768 vectors) for every example in the SST dataset. There are three files below for the training, dev, and test splits. \n",
        "\n",
        "If you haven't seen BERT before, it is a large neural network for natural language that maps arbitrary text to feature vectors. We recommend this [blog](https://jalammar.github.io/illustrated-bert/), which has a fantastic explanation. In practice, it is common to build on top of pretrained feature vectors rather than starting from the raw text. In this notebook, we will be training a small network on top of the BERT features."
      ],
      "metadata": {
        "id": "lnNr_zhVbw9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 17fCCxc0XrfCxLs9uUp9v1OY7v1Sc9pPP\n",
        "!gdown --id 18_KoA8kLhg_GIxzyjLqMID1I51397F1X\n",
        "!gdown --id 1koPWrlRKXV_nQmyGVHE7Z4X0VLrAjn-_"
      ],
      "metadata": {
        "id": "_mErydjgZpAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = torch.load('sst-roberta-train.pt')\n",
        "dev_features = torch.load('sst-roberta-dev.pt')\n",
        "test_features = torch.load('sst-roberta-test.pt')\n",
        "\n",
        "# check the shape\n",
        "print(train_features.size())"
      ],
      "metadata": {
        "id": "S0b6GZESZqvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a PyTorch Dataset that loads the SST data but takes two additional parameters: `data_noise` and `label_noise`.\n",
        "\n",
        "Both will be a number between 0 and 1. The parameter `data_noise` represents the probability of adding Gaussian noise to the data. The parameter `label_noise` represents the probability of flipping the label (e.g. map 0 to 1 and map 1 to 0)."
      ],
      "metadata": {
        "id": "YNoXKnAfZnqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flip(weight):\n",
        "  # You may find this function helpful below!\n",
        "  return np.random.rand() < weight\n",
        "\n",
        "class CorruptSSTBERT(Dataset):\n",
        "  \"\"\"\n",
        "  Your task is to fill in some of the class methods. \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data_noise = 0, label_noise = 0, split = 'train'):\n",
        "    super().__init__()\n",
        "    assert split in ['train', 'dev', 'test'], f\"Split {split} not supported.\"\n",
        "    self.features = torch.load(f'sst-roberta-{split}.pt').cpu()\n",
        "    if split == 'dev': split = 'validation'\n",
        "    self.split = split\n",
        "    self.data = load_dataset('sst', split = split)\n",
        "\n",
        "    self.data_noise = data_noise\n",
        "    self.label_noise = label_noise\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    feature = self.features[index]\n",
        "    # ================================\n",
        "    # FILL ME OUT\n",
        "    # \n",
        "    # With some probability, add noise from a Gaussian\n",
        "    # distribution with mean 0 and stdev 1.\n",
        "    # \n",
        "    # HINT: the `flip` function may be helpful. To draw random\n",
        "    # samples, use `torch.rand_like(...)`. \n",
        "    # \n",
        "    # Solution code is two lines.\n",
        "    # \n",
        "    # Pseudocode:\n",
        "    # --\n",
        "    # if flip is successful:\n",
        "    #   add noise to feature\n",
        "    # \n",
        "    # Type:\n",
        "    # --\n",
        "    # feature: torch.FloatTensor\n",
        "    # ================================\n",
        "\n",
        "    label = round(self.data[index]['label'])\n",
        "    # ================================\n",
        "    # FILL ME OUT\n",
        "    #\n",
        "    # With some probability, flip the label.\n",
        "    #\n",
        "    # Solution code is two lines.\n",
        "    # \n",
        "    # Pseudocode:\n",
        "    # --\n",
        "    # if flip is successful:\n",
        "    #   change label\n",
        "    # \n",
        "    # Type:\n",
        "    # --\n",
        "    # label: integer (0 or 1)\n",
        "    # ================================\n",
        "    return feature, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.data.num_rows"
      ],
      "metadata": {
        "id": "2slyu67CZsqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we create a `DataModule`. A DataModule is a PyTorch Lightning data structure that makes it easy \n",
        "for PyTorch Lightning's Trainer (which we use below) to interact with DataLoaders."
      ],
      "metadata": {
        "id": "mz0cXArqfCQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SSTDataModule(pl.LightningDataModule):\n",
        "  \"\"\"\n",
        "  This is largely a wrapper around three datasets and their respective data\n",
        "  loaders. This also sets the batch_size (default is 32).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data_noise = 0, label_noise = 0, batch_size: int = 32):\n",
        "    super().__init__()\n",
        "    # Call CorruptSSTBERT rather than SSTBERT\n",
        "    self.sst_train = CorruptSSTBERT(data_noise, label_noise, split = 'train')\n",
        "    self.sst_dev = CorruptSSTBERT(data_noise, label_noise, split = 'dev')\n",
        "    self.sst_test = CorruptSSTBERT(data_noise, label_noise, split = 'test')\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.sst_train, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.sst_dev, batch_size=self.batch_size)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.sst_test, batch_size=self.batch_size)\n",
        "\n",
        "  def predict_dataloader(self):\n",
        "    return DataLoader(self.sst_test, batch_size=self.batch_size)"
      ],
      "metadata": {
        "id": "6ULO0-JHZvRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the neural network to predict sentiment. This network will take as input features from BERT. "
      ],
      "metadata": {
        "id": "tGf6dn9pfTwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  \"\"\"\n",
        "  Feel free to customize me if you want to see the effects of \n",
        "  data quality using different neural networks. For the first \n",
        "  run, keep this simple (one linear layer aka logistic \n",
        "  regression) so that it is fast to run.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, 1)  # logistic regression\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return torch.sigmoid(self.fc1(x))"
      ],
      "metadata": {
        "id": "ZCyczPyYeqiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we define the PyTorch Lightning System. If you haven't used Lightning before, this is the meat of it. The system instantiates the model and optimizer and also requires the user to define functionalities for a training step and an evaluation step. The benefit of this standardization is less room for bugs. "
      ],
      "metadata": {
        "id": "dn6ZwpQxfocD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SSTSystem(pl.LightningModule):\n",
        "  \"\"\"\n",
        "  Defines a binary cross entropy loss between predicted\n",
        "  probabilites of positive sentiment and the true label. \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = MLP(768, 1) \n",
        "\n",
        "  def forward(self, features):\n",
        "    probs = self.model(features)\n",
        "    return probs\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "    return optimizer\n",
        "\n",
        "  def _common_step(self, batch, batch_idx):\n",
        "    features, labels = batch\n",
        "    labels = labels.unsqueeze(1).float()\n",
        "\n",
        "    probs = self.forward(features)\n",
        "    loss = F.binary_cross_entropy(probs, labels)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      preds = torch.round(probs.squeeze(1))\n",
        "      num_correct = torch.sum(preds == labels.squeeze(1)).item() \n",
        "      num_total = labels.size(0)\n",
        "      accuracy = num_correct / float(num_total)\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "    loss, acc = self._common_step(train_batch, batch_idx)\n",
        "    self.log('train_loss', loss)\n",
        "    self.log('train_acc', acc, prog_bar=True)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, dev_batch, batch_idx):\n",
        "    loss, acc = self._common_step(dev_batch, batch_idx)\n",
        "    self.log('dev_loss', loss)\n",
        "    self.log('dev_acc', acc, prog_bar=True)\n",
        "\n",
        "  def test_step(self, test_batch, batch_idx):\n",
        "    loss, acc = self._common_step(test_batch, batch_idx)\n",
        "    self.log('test_loss', loss)\n",
        "    self.log('test_acc', acc)\n",
        "\n",
        "  def predict_step(self, batch, batch_idx):\n",
        "    return self.forward(batch[0])"
      ],
      "metadata": {
        "id": "EJAr-NHPe-oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last thing to do before running experiments is to put the data module and the lightning system together. "
      ],
      "metadata": {
        "id": "SPok_We1gHDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, add a training function\n",
        "\n",
        "def train_test_pipeline(data_noise = 0, label_noise = 0):\n",
        "  seed_everything(42, use_cuda = True)\n",
        "  # pass this callback to the trainer - it saves the best model\n",
        "  # based on the dev loss. \n",
        "  checkpoint_callback = ModelCheckpoint(monitor = 'dev_loss')\n",
        "\n",
        "  # Initialize the data module, the lightning system and use \n",
        "  # Trainer to fit a model and evaluate performance using the \n",
        "  # best checkpoint found in training. This function should \n",
        "  # return the accuracy on the test set.\n",
        "  # \n",
        "  # Learn more about the Trainer class here:\n",
        "  # https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html\n",
        "  # \n",
        "  dm = SSTDataModule(\n",
        "    data_noise = data_noise, \n",
        "    label_noise = label_noise, \n",
        "    batch_size = 32)\n",
        "  model = SSTSystem()\n",
        "  trainer = Trainer(max_epochs = 20, gpus = 1, \n",
        "    callbacks = [checkpoint_callback])\n",
        "  trainer.fit(model, dm)\n",
        "  results = trainer.test(model, dm, ckpt_path = \"best\")\n",
        "\n",
        "  return results[0]['test_acc']"
      ],
      "metadata": {
        "id": "cRHG1PNVZwxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first study label noise. As we increase it from 0% to 100%, how does this effect test performance?"
      ],
      "metadata": {
        "id": "Ac4g_1lYZyvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A single model takes 2-3 minutes to train, so this job may take a few minutes.\n",
        "label_noise_results = []\n",
        "label_noise_choices = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "for label_noise in label_noise_choices:\n",
        "  test_acc = train_test_pipeline(label_noise = label_noise)\n",
        "  label_noise_results.append(test_acc)"
      ],
      "metadata": {
        "id": "8ujEpqlJZy_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(label_noise_choices, label_noise_results, '-o')\n",
        "plt.xlabel('Label Noise Prob', fontsize=16)\n",
        "plt.ylabel('Test Accuracy', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wNuoQx6VZ2Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that performance steadily decays as the amount of label noise increases. As expected, when the probability of flipping label is 50%, the performance drops to around 50% accuracy, which is the same as randomly guessing positive or negative sentiment. This makes sense since we are adding enough noise that the resulting label is random!\n",
        "\n",
        "The other important takeaway is that we can live with a little bit of noise. Even adding 10% of annotation noise only decreases performance by 5%. Real world annotators are not going to perfect and there will always be some annotation error. But a little may not be the end of the world. But too much noise, and performance will suffer greatly. That is why it is so important to ensure and protect data quality."
      ],
      "metadata": {
        "id": "KHjMe9DGZ4yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Question**: What do you think would happen if we set `p` to be bigger than 0.5? What do you expect performance to be if `p = 1`?\n",
        "\n",
        "> **Your Answer**: (Write your response here)."
      ],
      "metadata": {
        "id": "IgFq_j_jZ7A5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's study data noise, where we will corrupt the BERT features rather than the annotated labels. How does this impact performance? "
      ],
      "metadata": {
        "id": "-tqEuKDiZ8nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_noise_results = []\n",
        "data_noise_choices = [0, 0.01, 0.02, 0.05, 0.1]\n",
        "for data_noise in data_noise_choices:\n",
        "  test_acc = train_test_pipeline(data_noise = data_noise)\n",
        "  data_noise_results.append(test_acc)"
      ],
      "metadata": {
        "id": "2HJLIvp9Z5Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(data_noise_choices, data_noise_results, '-o')\n",
        "plt.xlabel('Data Noise Prob', fontsize=16)\n",
        "plt.ylabel('Test Accuracy', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WtwuqMiHaAM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can't directly compare the impact of data noise to label noise but at the very least we can see that a bit of noise added to the features impacts performance a lot. A 2% chance of adding noise drops performance by almost 15%. A big takeaway here is that noise in the data impacts model performance significantly. It is very important to remove noise as best as you can before giving data to a model. Just because you have a lot of data does not guarantee that the model will learn something useful if that data is noisy."
      ],
      "metadata": {
        "id": "RztoouyrZ_so"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What about images?\n",
        "\n",
        "Next, let's study the effects of corruption on an image corpus. Focusing on data noise, there are lot of ways we can manipulate images! We will try corrupting the raw image different transformations like changing the resolution, the saturation, or adding a watermark. \n",
        "\n",
        "To get started, let's download the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) and inspect one of the images to get a feel for the data. "
      ],
      "metadata": {
        "id": "7mmqe1axLS0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    './cifar', train=True, download = True, transform = transforms.ToTensor())\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    './cifar', train=False, download = True, transform = transforms.ToTensor())"
      ],
      "metadata": {
        "id": "IxZvBXyCK5X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each data point is a 32 by 32 pixel RGB image. Each label will be a number between 0 and 9, meaning there are 10 total classes in the CIFAR10 dataset. The goal of image classification is to predict which object is the image! "
      ],
      "metadata": {
        "id": "5cE6a3PNNgv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_dataset.__getitem__(0)  # get the first example!\n",
        "pil_image = transforms.ToPILImage()(image)\n",
        "plt.imshow(pil_image)\n",
        "print(f'its a frog! (label={label})')"
      ],
      "metadata": {
        "id": "Cy02QMn6ML_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Noise\n",
        "\n",
        "There is wide variety of image transformations that could be considered noise. Here we will study three different practical examples you might find in the real world: different image resolutions, image filteres, and camera angles. \n",
        "\n",
        "### Blurring \n",
        "We can simulate images of different resolutions by blurring images. "
      ],
      "metadata": {
        "id": "XyOU9jAKQLz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('original image')\n",
        "pil_image = transforms.ToPILImage()(image)\n",
        "plt.imshow(pil_image)\n",
        "plt.show()\n",
        "\n",
        "print('lightly blurred image')\n",
        "blurring = transforms.GaussianBlur(kernel_size=5, sigma=0.5)\n",
        "blurred_image = blurring(image)\n",
        "pil_image = transforms.ToPILImage()(blurred_image)\n",
        "plt.imshow(pil_image)\n",
        "plt.show()\n",
        "\n",
        "print('heavily blurred image')\n",
        "# increasing sigma applies more blurring\n",
        "blurring = transforms.GaussianBlur(kernel_size=5, sigma=5)\n",
        "blurred_image = blurring(image)\n",
        "pil_image = transforms.ToPILImage()(blurred_image)\n",
        "plt.imshow(pil_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B4WSIaWnRnp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Color transforms\n",
        "We can simulate image filters by transforming applying jitter to the color spectrums in images. "
      ],
      "metadata": {
        "id": "1MMgeAwhSfr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('original image')\n",
        "pil_image = transforms.ToPILImage()(image)\n",
        "plt.imshow(pil_image)\n",
        "plt.show()\n",
        "\n",
        "print('lightly jittered image')\n",
        "jitter = transforms.ColorJitter(\n",
        "    brightness=.1, hue=.05, contrast=.05, saturation=.05)\n",
        "jittered_image = jitter(image)\n",
        "pil_image = transforms.ToPILImage()(jittered_image)\n",
        "plt.imshow(pil_image)\n",
        "plt.show()\n",
        "\n",
        "print('heavily jittered image')\n",
        "# increasing brightness/hue/contrast/saturation applies more jitter\n",
        "jitter = transforms.ColorJitter(\n",
        "    brightness=.1, hue=.5, contrast=.5, saturation=.5)\n",
        "jittered_image = jitter(image)\n",
        "pil_image = transforms.ToPILImage()(jittered_image)\n",
        "plt.imshow(pil_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PV_fA3ayTCn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rotation\n",
        "We can simulate different camera angles by randomly rotating images.  \n"
      ],
      "metadata": {
        "id": "rhbg0WjySixX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('original image')\n",
        "pil_image = transforms.ToPILImage()(image)\n",
        "plt.imshow(pil_image)\n",
        "plt.show()\n",
        "\n",
        "print('lightly rotated image')\n",
        "rotation = transforms.RandomRotation(degrees=(-10, 10))\n",
        "rotated_image = rotation(image)\n",
        "pil_image = transforms.ToPILImage()(rotated_image)\n",
        "plt.imshow(pil_image)\n",
        "plt.show()\n",
        "\n",
        "print('heavily rotated image')\n",
        "# increasing degrees applies more rotation\n",
        "rotation = transforms.RandomRotation(degrees=(-180, 180))\n",
        "rotated_image = rotation(image)\n",
        "pil_image = transforms.ToPILImage()(rotated_image)\n",
        "plt.imshow(pil_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0NSsvHkoSt7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Modeling\n",
        "\n",
        "We will be training a small convolutional neural network to classify objects \n",
        "from the raw image. Unlike the NLP example, we won't be using pretrained features this time around. "
      ],
      "metadata": {
        "id": "GgLhs3QGNrcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  \"\"\"\n",
        "  A convolutional neural network for images.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    # The output will be a vector of 10 real numbers representing\n",
        "    # `logits` for each of the 10 classes. \n",
        "    return x"
      ],
      "metadata": {
        "id": "qlEQ2OsuM5pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of using the CNN (untrained) on a data point."
      ],
      "metadata": {
        "id": "W6bF2aHUOcbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, _ = train_dataset.__getitem__(0)\n",
        "inputs = image.unsqueeze(0)  # shape: 1 x 3 x 32 x 32\n",
        "\n",
        "with torch.no_grad():\n",
        "  model = CNN()\n",
        "  logits = model(inputs)  # shape: 1 x 10\n",
        "  print(logits.squeeze())"
      ],
      "metadata": {
        "id": "e2SK8Y9QM-R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These predictions represent unnormalized probabilities for each of the 10 classes. To normalize them, we can use the softmax function."
      ],
      "metadata": {
        "id": "GS9DbXI-OwDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probs = F.softmax(logits, dim=1)\n",
        "print(probs.squeeze())"
      ],
      "metadata": {
        "id": "LHLNqBTSOp8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, the model thinks there is a ~9-10% chance this image is a frog. Since it isn't a trained model, it makes sense that this is near chance probability (10% since there are 10 classes). \n",
        "\n",
        "## Setup\n",
        "\n",
        "Okay! That's enough review of computer vision. We are ready to simulate the effects of these different image corruptions. As we did before, we will setup (1) a `Dataset` class to enable to to toggle the amount of corruption, (2) a DataModule, and (3) a Lightning System. \n",
        "\n",
        "We will provide the code for (1) but we will need your help with (2) and (3)! Review the DataModule and PyTorch Lightning system from the NLP example above. We provide scaffold code below that you will need to complete."
      ],
      "metadata": {
        "id": "qziYzZz2O9N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CorruptCIFAR10(Dataset):\n",
        "  \"\"\"Dataset for loading corrupted CIFAR10 images. \n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  noise_type (str, default = blur): the type of data corruption. \n",
        "    Choices: blur, jitter, rotate, none\n",
        "\n",
        "  noise_level (str, default = light): the amount of corruption\n",
        "    Choices: light, medium, heavy\n",
        "\n",
        "    Note: unlike the NLP example where we continuously toggle the amount of \n",
        "    corruption, here we just chose between a few modes!\n",
        "\n",
        "  train (bool, default = True): train or test split?\n",
        "  \"\"\"\n",
        "  def __init__(self, noise_type = 'blur', noise_level = 'light', train = True):\n",
        "    super().__init__()\n",
        "    assert noise_type in ['none', 'blur', 'jitter', 'rotate']\n",
        "    assert noise_level in ['light', 'medium', 'heavy']\n",
        "    \n",
        "    self.noise_type = noise_type\n",
        "    self.noise_level = noise_level\n",
        "\n",
        "    self.dataset = datasets.CIFAR10('./cifar', \n",
        "      train = train, download = True, transform = self.get_transforms())\n",
        "\n",
        "  def get_transforms(self):\n",
        "    if self.noise_type == 'none':\n",
        "      # do nothing!\n",
        "      return transforms.ToTensor()\n",
        "\n",
        "    elif self.noise_type == 'blur':\n",
        "      if self.noise_level == 'light':\n",
        "        max_sigma = .5\n",
        "      elif self.noise_level == 'medium':\n",
        "        max_sigma = 2\n",
        "      elif self.noise_level == 'heavy':\n",
        "        max_sigma = 5\n",
        "      return transforms.Compose([\n",
        "        transforms.GaussianBlur(kernel_size=5, sigma=max_sigma),\n",
        "        transforms.ToTensor()\n",
        "      ])\n",
        "\n",
        "    elif self.noise_type == 'jitter':\n",
        "      if self.noise_level == 'light':\n",
        "        max_jitter = .05\n",
        "      elif self.noise_level == 'medium':\n",
        "        max_jitter = .2\n",
        "      elif self.noise_level == 'heavy':\n",
        "        max_jitter = .5\n",
        "      return transforms.Compose([\n",
        "        transforms.ColorJitter(brightness=.1, \n",
        "          hue=max_jitter, contrast=max_jitter, saturation=max_jitter),\n",
        "        transforms.ToTensor()\n",
        "      ])\n",
        "\n",
        "    elif self.noise_type == 'rotate':\n",
        "      if self.noise_level == 'light':\n",
        "        max_degree = 10\n",
        "      elif self.noise_level == 'medium':\n",
        "        max_degree = 45\n",
        "      elif self.noise_level == 'heavy':\n",
        "        max_degree = 180\n",
        "      return transforms.Compose([  # first rotate, then transform\n",
        "        transforms.RandomRotation(degrees=(-max_degree, max_degree)),\n",
        "        transforms.ToTensor()\n",
        "      ])\n",
        "  def __getitem__(self, index):\n",
        "    image, label = self.dataset.__getitem__(index)\n",
        "    return image, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)"
      ],
      "metadata": {
        "id": "g-lB9zqPQZg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "  \"\"\"Wrapper around the CorruptCIFAR10 class.\"\"\"\n",
        "\n",
        "  def __init__(self, \n",
        "    noise_type = 'blur', noise_level = 'light', batch_size: int = 32):\n",
        "    super().__init__()\n",
        "    self.cifar_train = None\n",
        "    self.cifar_test = None  # no dev split for CIFAR10\n",
        "    # ============================\n",
        "    # FILL ME OUT\n",
        "    # \n",
        "    # Populate `cifar_train` with a corrupted dataset using the `noise_level`\n",
        "    # and `noise_type` specified above. Populate `cifar_test` with a \n",
        "    # *uncorrupted* dataset that we will use to measure performance. \n",
        "    # What `noise_level` should be used for an uncorrupted dataset? \n",
        "    # \n",
        "    # Pseudocode:\n",
        "    # --\n",
        "    # self.cifar_train = ...\n",
        "    # self.cifar_test = ...\n",
        "    #  \n",
        "    # Type:\n",
        "    # --\n",
        "    # self.cifar_*: CorruptCIFAR10\n",
        "    # ============================\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.cifar_train, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.cifar_test, batch_size=self.batch_size)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.cifar_test, batch_size=self.batch_size)\n",
        "\n",
        "  def predict_dataloader(self):\n",
        "    return DataLoader(self.cifar_test, batch_size=self.batch_size)"
      ],
      "metadata": {
        "id": "1QvwRbmWV1D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10System(pl.LightningModule):\n",
        "  \"\"\"\n",
        "  Defines a cross entropy loss between predicted probabilites of the object \n",
        "  class and the true label from CIFAR10. \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = None\n",
        "    # ============================\n",
        "    # FILL ME OUT\n",
        "    # \n",
        "    # Initialize the convolutional network into the `self.model` variable.\n",
        "    # \n",
        "    # Solution code is one line of code.\n",
        "    # \n",
        "    # Pseudocode:\n",
        "    # --\n",
        "    # self.model = ...\n",
        "    # \n",
        "    # Type:\n",
        "    # --\n",
        "    # self.model: nn.Module\n",
        "    # ============================\n",
        "\n",
        "  def forward(self, features):\n",
        "    # remember, these are unnormalized!\n",
        "    logits = self.model(features)\n",
        "    return logits\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "    return optimizer\n",
        "\n",
        "  def _common_step(self, batch, batch_idx):\n",
        "    features, labels = batch\n",
        "    logits = self.forward(features)\n",
        "\n",
        "    loss = None\n",
        "    # ============================\n",
        "    # FILL ME OUT\n",
        "    # \n",
        "    # Compute the cross entropy loss between logits and labels. Note \n",
        "    # that we are not longer in a binary classification task. Make sure\n",
        "    # to use the right version of cross entropy.\n",
        "    # \n",
        "    # Solution code is one line of code.\n",
        "    # \n",
        "    # Pseudocode:\n",
        "    # --\n",
        "    # loss = ...\n",
        "    # \n",
        "    # Type:\n",
        "    # --\n",
        "    # loss: torch.FloatTensor\n",
        "    #   shape: 1\n",
        "    # ============================\n",
        "\n",
        "    with torch.no_grad():\n",
        "      preds = torch.argmax(logits, dim=1)  # biggest logit = biggest prob\n",
        "      num_correct = torch.sum(preds == labels).item() \n",
        "      num_total = labels.size(0)\n",
        "      accuracy = num_correct / float(num_total)\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "    loss, acc = self._common_step(train_batch, batch_idx)\n",
        "    self.log('train_loss', loss)\n",
        "    self.log('train_acc', acc, prog_bar=True)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, dev_batch, batch_idx):\n",
        "    loss, acc = self._common_step(dev_batch, batch_idx)\n",
        "    self.log('dev_loss', loss)\n",
        "    self.log('dev_acc', acc, prog_bar=True)\n",
        "\n",
        "  def test_step(self, test_batch, batch_idx):\n",
        "    loss, acc = self._common_step(test_batch, batch_idx)\n",
        "    self.log('test_loss', loss)\n",
        "    self.log('test_acc', acc)\n",
        "\n",
        "  def predict_step(self, batch, batch_idx):\n",
        "    return self.forward(batch[0])"
      ],
      "metadata": {
        "id": "BGBOELN3Y3p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put it together with a pipeline function to call the system you wrote!"
      ],
      "metadata": {
        "id": "Pi6m-E_NZx0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_pipeline(noise_type = 'blur', noise_level = 'light'):\n",
        "  seed_everything(42, use_cuda = True)\n",
        "  dm = CIFAR10DataModule(noise_level = noise_level, noise_type = noise_type,\n",
        "    batch_size = 64)\n",
        "  model = CIFAR10System()\n",
        "  trainer = Trainer(max_epochs = 5, gpus = 1)\n",
        "  trainer.fit(model, dm)\n",
        "  results = trainer.test(model, dm)\n",
        "\n",
        "  return results[0]['test_acc']"
      ],
      "metadata": {
        "id": "edgGEhswZhOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Experiments!\n",
        "\n",
        "Okay, we are all setup to run our experiments. Let's start with blur and vary the noise level from light to heavy to check the performance. "
      ],
      "metadata": {
        "id": "OD43NySTaMDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Each model takes roughly 1-3 minutes to train. Go take walk or get a coffee! \n",
        "# Come back in 10 minutes :) \n",
        "no_blur_acc = train_test_pipeline(noise_type = 'none')  # baseline\n",
        "light_blur_acc = train_test_pipeline(noise_type = 'blur', noise_level = 'light')\n",
        "medium_blur_acc = train_test_pipeline(noise_type = 'blur', noise_level = 'medium')\n",
        "heavy_blur_acc = train_test_pipeline(noise_type = 'blur', noise_level = 'heavy')"
      ],
      "metadata": {
        "id": "ZRpmVx-yaIhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the results on a bar plot."
      ],
      "metadata": {
        "id": "3wDSWhbggbXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.bar(['none', 'light', 'medium', 'heavy'], \n",
        "        [no_blur_acc, light_blur_acc, medium_blur_acc, heavy_blur_acc])\n",
        "plt.ylim(0.4, 0.6)\n",
        "plt.title('Effect of blurring on performance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tHYKu7tNgiHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run the other two transformations as well, which will help us compare the effects of different noise types."
      ],
      "metadata": {
        "id": "8dyeITw9c9Sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run me for jitter experiments!\n",
        "no_jitter_acc = train_test_pipeline(noise_type = 'none')  # baseline\n",
        "light_jitter_acc = train_test_pipeline(noise_type = 'jitter', noise_level = 'light')\n",
        "medium_jitter_acc = train_test_pipeline(noise_type = 'jitter', noise_level = 'medium')\n",
        "heavy_jitter_acc = train_test_pipeline(noise_type = 'jitter', noise_level = 'heavy')"
      ],
      "metadata": {
        "id": "ajIJuM4nam-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.bar(['none', 'light', 'medium', 'heavy'], \n",
        "        [no_blur_acc, light_jitter_acc, medium_jitter_acc, heavy_jitter_acc])\n",
        "plt.ylim(0.4, 0.6)\n",
        "plt.title('Effect of rotation on performance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fjq_E3qSjomm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run me for rotation experiments!\n",
        "no_rotate_acc = train_test_pipeline(noise_type = 'none')  # baseline\n",
        "light_rotate_acc = train_test_pipeline(noise_type = 'rotate', noise_level = 'light')\n",
        "medium_rotate_acc = train_test_pipeline(noise_type = 'rotate', noise_level = 'medium')\n",
        "heavy_rotate_acc = train_test_pipeline(noise_type = 'rotate', noise_level = 'heavy')"
      ],
      "metadata": {
        "id": "yG90H4kudIt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.bar(['none', 'light', 'medium', 'heavy'], \n",
        "        [no_rotate_acc, light_rotate_acc, medium_rotate_acc, heavy_rotate_acc])\n",
        "plt.ylim(0.4, 0.6)\n",
        "plt.title('Effect of rotation on performance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sdlU1Y2UiWCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "First, we can observe that for all three noise families, adding heavy noise decreases performance dramatically. The exact amount varies by noise type: for instance, adding heavy rotation reduces performance by 15% whereas heavy jitter only reduces performance by 2-3%.  Second, we observe that the model is relatively resistant to light noise, losing only 0-2% in accuracy. In blurring, almost no loss comes from light noise. Third, sometimes, we might observe that additional noise benefits performance: this is subtle and comes from the effects of data augmentation. Carefully added noise can help prevent overfitting. \n",
        "\n",
        "The overall takeaway is that like the NLP example, noise in images can also hurt model performance. However, in both, a light amount of noise is manageable, and can even be interpreted as a benefit. "
      ],
      "metadata": {
        "id": "ieGgmwfJmPZI"
      }
    }
  ]
}